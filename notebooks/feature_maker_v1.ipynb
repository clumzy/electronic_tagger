{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from p_tqdm import p_map\n",
    "import librosa\n",
    "import librosa.display\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LISTAGE DES FICHIERS CONTENUS DANS LE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_list(location = \"D:\\George/Documents/Code/clumzy/tag_my_techno/datasets\", max_samples = 250, random_state = 420):\n",
    "    \"\"\"\n",
    "    Retrieves a list of audio files given a root directory, ensuring a certain number of samples per subdirectory.\n",
    "\n",
    "    Args:\n",
    "        location (str): Path to the root directory containing audio files within subdirectories.\n",
    "        max_samples (int): Maximum number of audio samples to retrieve per subdirectory.\n",
    "        random_state (int): Seed for random operations to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A structured array where each row contains:\n",
    "            - File path of the audio sample.\n",
    "            - Genre or subdirectory name.\n",
    "            - Number of times the sample appears to adjust for offset.\n",
    "    \"\"\"\n",
    "    audio_files = np.empty((0,3))\n",
    "    # LES EXTENSIONS QUE L'ON CHERCHE\n",
    "    exts = ['*.mp3', '*.aiff', '*.flac','*.wav']\n",
    "    # LES DIFFERENTS SOUS-DOSSIERS (ON NE VERIFIE PAS QU'ILS CONTIENNENT DES ELEMENTS)\n",
    "    sub_directories = [f.path for f in os.scandir(location) if f.is_dir()]\n",
    "    for directory in sub_directories:\n",
    "        # ON LISTE LES FICHIERS AUDIO TROUVES DANS LE SUB DIR\n",
    "        sub_dir_files = [f for ext in exts for f in glob(os.path.join(directory, ext))]\n",
    "        if len(sub_dir_files) > 0:\n",
    "            # ON SEED\n",
    "            random.seed(random_state)\n",
    "            # ON MELANGE\n",
    "            random.shuffle(sub_dir_files)\n",
    "            # ON DUPLIQUE LES FICHIERS SI IL Y EN A PAS ASSEZ\n",
    "            while len(sub_dir_files) < max_samples : sub_dir_files = sub_dir_files + sub_dir_files[:max_samples-len(sub_dir_files)]\n",
    "            # ON RECUPERE DES SAMPLES\n",
    "            sub_dir_samples = random.sample(sub_dir_files, k=max_samples)\n",
    "            for sample in sub_dir_samples:\n",
    "                #ON RENVOIE\n",
    "                # 0 - CHEMIN DU FICHIER\n",
    "                # 1 - GENRE\n",
    "                # 2 - NOMBRE DE FOIS OU IL APPARAIT AFIN DE BIEN METTRE UN OFFSET\n",
    "                audio_files = np.append(audio_files, [[sample, os.path.basename(directory), np.count_nonzero(audio_files[-len(sub_dir_files):] == sample)]], axis=0)\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE MAKER FUNCTION & SCALER FUNCTION\n",
    "\n",
    "Ces fonctions nous servent à scaler et créer **l'image feature** pour un fichier donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydub as pydub\n",
    "import librosa\n",
    "import warnings\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def rgb_transform(data):\n",
    "    \"\"\"Une fonction qui prend en entrée une image au format Array en RGB, et qui va normaliser\n",
    "    selon une formule MinMax scalée sur 255.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.array): Une image au format numpy Array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.array: L'image normalisée.\n",
    "    \"\"\"\n",
    "    return (((data+abs(data.min()))/(data+abs(data.min())).max())*255).astype(np.uint8)\n",
    "\n",
    "def get_from_pydub(file, num_blocks=4, num_samples = 8, sample_length=2, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    Extracts segments from an audio file and calculates their BPM (Beats Per Minute) to determine measure length for music genre classification.\n",
    "\n",
    "    This function reads an audio file, converts it to mono channel, sets a specified sample rate, normalizes the audio data,\n",
    "    calculates the BPM, determines the length of a musical measure in seconds, and then extracts segments of audio\n",
    "    based on the calculated measure length. It returns these segments along with the measure length for further processing.\n",
    "\n",
    "    Parameters:\n",
    "    - file (str): Path to the audio file to process.\n",
    "    - num_blocks (int, optional): Number of blocks to divide the song into. Defaults to 5.\n",
    "    - num_samples (int, optional): Number of samples to extract per block. Defaults to 4.\n",
    "    - sample_length (int, optional): Length of each sample in measures. Defaults to 2.\n",
    "    - sample_rate (int, optional): Sample rate for the audio file. Defaults to 44100 Hz.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two elements:\n",
    "        - list: Segments extracted from the audio file, ready for feature extraction.\n",
    "        Each segment is a numpy array representing a portion of the audio waveform.\n",
    "        - int: Length of a musical measure in samples, calculated based on the BPM of the song.\n",
    "\n",
    "    The function processes the audio file to prepare it for genre classification by extracting relevant segments\n",
    "    based on musical measures, which helps in aligning the analysis with the rhythmic structure of the music.\n",
    "    It uses PyDub for initial processing and calculates BPM using a custom function to understand the tempo,\n",
    "    aiding in the segmentation process.\n",
    "\n",
    "    Example:\n",
    "    >>> segments, measure_length = get_from_pydub(\"path/to/audio/file.mp3\")\n",
    "    >>> print(f\"Number of segments: {len(segments)}, Measure length: {measure_length}\")\n",
    "\n",
    "    Note: The function assumes the audio file is stereo and converts it to mono for simplicity.\n",
    "          It also normalizes the audio waveform to a range between -1 and 1 for consistency across files.\n",
    "    \"\"\"\n",
    "    song:pydub.AudioSegment = pydub.AudioSegment.from_file(file).set_channels(1)\n",
    "    # SET SAMPLE RATE TO sample_rate\n",
    "    song = song.set_frame_rate(sample_rate)\n",
    "    song = np.array(song.get_array_of_samples())\n",
    "    # normalize audio to -1, 1 in float32 for numpy\n",
    "    song = song.astype(np.float32) / (2**15 - 1)\n",
    "    bpm = calculate_bpm(song)\n",
    "    #get length of a measure in seconds\n",
    "    measure_length = int(60 / bpm * 4 * sample_rate)\n",
    "    #convert length to scale of numpy indices\n",
    "    start_points = np.linspace(\n",
    "        start = 0, \n",
    "        stop = len(song)-1,\n",
    "        endpoint= False, \n",
    "        num = num_blocks, \n",
    "        dtype=int) + int(len(song) / 6 / 2)\n",
    "    segments = [[song[s + n * sample_length * measure_length:s + (n + 1) * sample_length * measure_length] for n in range(num_samples)] for s in start_points]\n",
    "    return segments, measure_length\n",
    "\n",
    "def segments_to_img(segments, hop_length=1024, sample_rate=44100):\n",
    "    \"\"\"Transforms each segment of audio into an image.\n",
    "\n",
    "    Args:\n",
    "        segments (list of list of np.array): Segments of audio to transform.\n",
    "        hop_length (int, optional): The hop length for the analysis. Defaults to 1024.\n",
    "        sample_rate (int, optional): The sample rate of the audio. Defaults to 44100.\n",
    "\n",
    "    Returns:\n",
    "        list of list of np.array: Images corresponding to each audio segment.\n",
    "    \"\"\"\n",
    "    def transform_segment_to_img(song_extract):\n",
    "        \"\"\"Transforms a single audio segment into an image.\"\"\"\n",
    "        warnings.filterwarnings('ignore')\n",
    "        HOP_LENGTH = hop_length\n",
    "        SAMPLING_RATE = sample_rate\n",
    "\n",
    "        # Calculate Constant-Q\n",
    "        constant_q_temp = librosa.cqt(song_extract, hop_length=HOP_LENGTH, sr=SAMPLING_RATE)\n",
    "        constant_q = librosa.amplitude_to_db(np.abs(constant_q_temp))\n",
    "    \n",
    "        # Calculate MFCC\n",
    "        mfcc_song = librosa.feature.mfcc(y=song_extract, n_mfcc=26, sr=SAMPLING_RATE, lifter=512, hop_length=HOP_LENGTH)\n",
    "\n",
    "        # Calculate Chromagram\n",
    "        chromacens = librosa.feature.chroma_cens(y=song_extract, sr=SAMPLING_RATE, hop_length=HOP_LENGTH, n_chroma=36, win_len_smooth=5, C=constant_q_temp)\n",
    "        #CREATION DE LA SHAPE FINALE A PARTIR DES PLUS GRANDES VALEURS DE NOS TROIS FEATURES\n",
    "        # Create final image shape\n",
    "        IM_HEIGHT = max(constant_q.shape[0], mfcc_song.shape[0], chromacens.shape[0])\n",
    "        IM_WIDTH = max(constant_q.shape[1], mfcc_song.shape[1], chromacens.shape[1])\n",
    "        IM_SHAPE = (IM_HEIGHT, IM_WIDTH)\n",
    "\n",
    "        r = rgb_transform(resize(constant_q, IM_SHAPE, anti_aliasing=None, mode=\"reflect\", order=0)).astype(np.uint8)\n",
    "        g = rgb_transform(resize(mfcc_song, IM_SHAPE, anti_aliasing=None, mode=\"reflect\", order=0)).astype(np.uint8)\n",
    "        b = rgb_transform(resize(chromacens, IM_SHAPE, anti_aliasing=None, mode=\"reflect\", order=0)).astype(np.uint8)\n",
    "\n",
    "        rgb = np.dstack((r, g, b)).astype(np.uint8)\n",
    "        return rgb\n",
    "\n",
    "    # Transform each segment in the list of lists\n",
    "    images = [[transform_segment_to_img(segment) for segment in block] for block in segments]\n",
    "    return images\n",
    "\n",
    "def calculate_bpm(audio_series, sr=44100):\n",
    "    \"\"\"\n",
    "    Calculates BPM using librosa's beat tracking.\n",
    "    \"\"\"\n",
    "    tempo, _ = librosa.beat.beat_track(y=audio_series, sr=sr)\n",
    "    return round(tempo, 2)\n",
    "\n",
    "def plot_segments_images(images):\n",
    "    \"\"\"\n",
    "    Plots the images of audio segments in a grid format with blocks as x and segments as y,\n",
    "    enhanced for better visual distinction and aesthetics.\n",
    "\n",
    "    Args:\n",
    "        images (list of list of np.array): Images corresponding to each audio segment.\n",
    "    \"\"\"\n",
    "    num_blocks = len(images)\n",
    "    num_segments = len(images[0]) if num_blocks > 0 else 0\n",
    "\n",
    "    # Adjust figure size based on the number of blocks and segments for better readability\n",
    "    figsize = (num_blocks * 4, num_segments * 3) if num_blocks > 5 else (num_blocks * 3, num_segments * 2)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_segments, num_blocks, figsize=figsize, constrained_layout=True)\n",
    "\n",
    "    title_bg_colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink', 'lightcyan', 'lightgoldenrodyellow', 'lightgray', 'lightsteelblue', 'lightseagreen']\n",
    "\n",
    "    for i, block in enumerate(images):\n",
    "        for j, img in enumerate(block):\n",
    "            # Select the current axis object based on the number of segments\n",
    "            ax = axes[j, i] if num_segments > 1 else axes[i]\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # Remove axis ticks for cleaner look\n",
    "            ax.axis('off')\n",
    "            # Enhanced title with block and segment information\n",
    "            title_bg_color = title_bg_colors[i % len(title_bg_colors)]\n",
    "            ax.set_title(f'Block {i+1}, Segment {j+1}', fontsize=10, pad=10, backgroundcolor=title_bg_color)\n",
    "    \n",
    "    # Add a grid to the plot\n",
    "    for ax in axes.flat:\n",
    "        ax.grid(True, which='both', color='gray', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Adjust layout to prevent overlap and ensure tight spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    import numpy as np\n",
    "import pydub\n",
    "\n",
    "def get_slice_indices_from_pydub(file, num_blocks=4, num_samples=8, sample_length=2, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    Calculates the slice indices for segments from an audio file based on the BPM (Beats Per Minute) to determine measure length for music genre classification.\n",
    "\n",
    "    This function reads an audio file, converts it to mono channel, sets a specified sample rate, normalizes the audio data,\n",
    "    calculates the BPM, determines the length of a musical measure in seconds, and then calculates the slice indices\n",
    "    for segments of audio based on the calculated measure length.\n",
    "\n",
    "    Parameters:\n",
    "    - file (str): Path to the audio file to process.\n",
    "    - num_blocks (int, optional): Number of blocks to divide the song into. Defaults to 5.\n",
    "    - num_samples (int, optional): Number of samples to extract per block. Defaults to 4.\n",
    "    - sample_length (int, optional): Length of each sample in measures. Defaults to 2.\n",
    "    - sample_rate (int, optional): Sample rate for the audio file. Defaults to 44100 Hz.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lists containing the start and end indices for each segment.\n",
    "    \"\"\"\n",
    "    song = pydub.AudioSegment.from_file(file).set_channels(1)\n",
    "    song = song.set_frame_rate(sample_rate)\n",
    "    song_len = len(song)  # Duration in ms\n",
    "    song = np.array(song.get_array_of_samples())\n",
    "    song = song.astype(np.float32) / (2**15 - 1)  # Normalize audio to -1, 1 in float32 for numpy\n",
    "\n",
    "    bpm = calculate_bpm(song)\n",
    "    measure_length = int(60 / bpm * 4 * sample_rate)  # Length of a measure in samples\n",
    "\n",
    "    start_points = np.linspace(\n",
    "        start=0,\n",
    "        stop=len(song) - 1,\n",
    "        endpoint=False,\n",
    "        num=num_blocks,\n",
    "        dtype=int\n",
    "    ) + int(len(song) / 6 / 2)\n",
    "\n",
    "    slice_indices = [\n",
    "        [(s + n * sample_length * measure_length, s + (n + 1) * sample_length * measure_length) for n in range(num_samples)]\n",
    "        for s in start_points\n",
    "    ]\n",
    "\n",
    "    return slice_indices\n",
    "\n",
    "def plot_waveform_with_slices(file, slice_indices, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    Plots the waveform of the song with overlaid blocks, segments, and samples, recolored based on the block.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path to the audio file to process.\n",
    "        slice_indices (list): List of lists containing the start and end indices for each segment.\n",
    "        sample_rate (int, optional): Sample rate for the audio file. Defaults to 44100 Hz.\n",
    "    \"\"\"\n",
    "    # Load the song\n",
    "    song = pydub.AudioSegment.from_file(file).set_channels(1)\n",
    "    song = song.set_frame_rate(sample_rate)\n",
    "    song = np.array(song.get_array_of_samples())\n",
    "    song = song.astype(np.float32) / (2**15 - 1)  # Normalize audio to -1, 1 in float32 for numpy\n",
    "\n",
    "    # Define a list of darker colors for the blocks\n",
    "    block_colors = ['darkred', 'darkblue', 'darkgreen', 'darkorange', 'darkmagenta', 'darkcyan', 'darkgoldenrod', 'dimgray', 'darkslateblue', 'darkseagreen']\n",
    "\n",
    "    # Plot the waveform\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(song, color='black', alpha=0.6, label='Waveform')\n",
    "\n",
    "    # Overlay the blocks, segments, and samples\n",
    "    for block_idx, block in enumerate(slice_indices):\n",
    "        block_color = block_colors[block_idx % len(block_colors)]\n",
    "        for segment_idx, (start, end) in enumerate(block):\n",
    "            plt.axvspan(start, end, color=block_color, alpha=0.5, label=f'Block {block_idx+1}, Segment {segment_idx+1}' if segment_idx == 0 else \"\")\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Waveform with Overlaid Blocks, Segments, and Samples')\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def file_to_imgs(\n",
    "        file, \n",
    "        num_blocks = 4, \n",
    "        num_samples = 8, \n",
    "        sample_length = 2, \n",
    "        sample_rate=44100, \n",
    "        hop_length=512):\n",
    "    \"\"\"\n",
    "    Converts an audio file into a list of image representations of its segments.\n",
    "\n",
    "    This function takes an audio file and processes it to extract segments based on musical measures. \n",
    "    It then transforms each segment into an image representation using a combination of Constant-Q Transform (CQT),\n",
    "    Mel-Frequency Cepstral Coefficients (MFCC), and Chromagram features. The resulting images are returned as a \n",
    "    list of lists, where each inner list represents a block of segments.\n",
    "\n",
    "    Parameters:\n",
    "    - file (str): Path to the audio file to process.\n",
    "    - num_blocks (int, optional): Number of blocks to divide the song into. Defaults to 4.\n",
    "    - num_samples (int, optional): Number of samples (segments) to extract per block. Defaults to 8.\n",
    "    - sample_length (int, optional): Length of each sample in measures. Defaults to 2.\n",
    "    - sample_rate (int, optional): Sample rate for the audio file. Defaults to 44100 Hz.\n",
    "    - hop_length (int, optional): The hop length (number of samples between successive frames) for the \n",
    "      feature extraction. Defaults to 512.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lists containing the image representations of the audio segments. Each inner list represents\n",
    "      a block of segments.\n",
    "\n",
    "    The function first calls the `get_from_pydub` function to extract the audio segments based on the provided parameters.\n",
    "    It then passes the extracted segments to the `segments_to_img` function, which transforms each segment into an image\n",
    "    representation using a combination of CQT, MFCC, and Chromagram features.\n",
    "\n",
    "    The resulting images are returned as a list of lists, where each inner list represents a block of segments. This allows\n",
    "    for organizing the segments into logical groups for further analysis or processing.\n",
    "\n",
    "    Example:\n",
    "    >>> audio_file = \"path/to/audio/file.mp3\"\n",
    "    >>> images = file_to_imgs(audio_file, num_blocks=6, num_samples=10, sample_length=4)\n",
    "    >>> print(f\"Number of blocks: {len(images)}, Segments per block: {len(images[0])}\")\n",
    "\n",
    "    Note: The function assumes the audio file is stereo and converts it to mono for simplicity. It also normalizes the \n",
    "    audio waveform to a range between -1 and 1 for consistency across files.\n",
    "    \"\"\"\n",
    "    segments, measure_length = get_from_pydub(file,\n",
    "        num_blocks=num_blocks,\n",
    "        num_samples=num_samples,\n",
    "        sample_length=sample_length,\n",
    "        sample_rate=sample_rate)\n",
    "    processed_segments = segments_to_img(\n",
    "        segments=segments,\n",
    "        sample_rate=44100,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    return processed_segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXEMPLE D'UNE CREATION DE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SONG_NUM = 2\n",
    "audio_files = get_audio_list(max_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_segments = file_to_imgs(audio_files[SONG_NUM,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_segments = file_to_imgs(audio_files[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = get_slice_indices_from_pydub(file=audio_files[SONG_NUM, 0])\n",
    "plot_waveform_with_slices(audio_files[SONG_NUM, 0], slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_segments[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `processed_segments` is the output from `segments_to_img`\n",
    "plot_segments_images(processed_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATION DES FEATURES POUR LA LISTE DES MUSIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = get_audio_list(max_samples=500)\n",
    "HOW_MANY = len(audio_files)\n",
    "X = np.array(p_map(\n",
    "    song_to_img,\n",
    "    audio_files[:, 0],\n",
    "    [1024]*HOW_MANY,\n",
    "    [10]*HOW_MANY,\n",
    "    [3]*HOW_MANY,\n",
    "    [44100]*HOW_MANY,\n",
    "    audio_files[:, 2],\n",
    "    [MAX_OFFSET]*HOW_MANY), dtype=np.uint8)\n",
    "print(X.shape)\n",
    "\n",
    "y = audio_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPORT DES FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(X, open( os.path.join(\"/home/george/code/clumzy/electronic_tagger/images\",\"images3_big.pkl\"), \"wb\" ) )\n",
    "pickle.dump(y, open( os.path.join(\"/home/george/code/clumzy/electronic_tagger/images\",\"target3_big.pkl\"), \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X[:2625]\n",
    "X2 = X[2625:5250]\n",
    "X3 = X[5250:7875]\n",
    "X4 = X[7875:]\n",
    "pickle.dump(X1, open( os.path.join(\"/home/george/code/clumzy/electronic_tagger/images\",\"images3_big_part1.pkl\"), \"wb\" ) )\n",
    "pickle.dump(X2, open( os.path.join(\"/home/george/code/clumzy/electronic_tagger/images\",\"images3_big_part2.pkl\"), \"wb\" ) )\n",
    "pickle.dump(X3, open( os.path.join(\"/home/george/code/clumzy/electronic_tagger/images\",\"images3_big_part3.pkl\"), \"wb\" ) )\n",
    "pickle.dump(X4, open( os.path.join(\"/home/george/code/clumzy/electronic_tagger/images\",\"images3_big_part4.pkl\"), \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
